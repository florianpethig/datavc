---
title: "Exploring Data Version Control for Research"
author: "Florian Pethig"
date: "Last updated: `r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

## What is data version control?

Real world data, such as logs, comments, and reviews, are usually messy and require substantial processing. Sometimes, this data may also be updated during the project, e.g., when data collection is ongoing (as is the case for many COVID-19 working papers). This can result in many different versions of datasets throughout the research project (e.g., v2, v3, featureXpresent, featureYabsent) and may lead to confusing and hardly reproducible results when looking at an analysis that has been conducted a few weeks back. Moreover, the data that researchers in social sciences and other fields have at hand become increasingly large, often exceeding several GBs. Simply running the entire pre-processing pipeline when compiling an R Markdown document can take hours to complete. Versioning intermediate datasets with Git is often not feasible due to storage restrictions of code repositories and technical difficulties to efficiently handle large files.

## Are researchers already using data version control?

I distributed a survey through the channels of the fellow program (n=15 completed answers) to understand current data research practices among other researchers from various disciplines (a summary of the results is available [here](https://de.wikiversity.org/wiki/Wikiversity:Fellow-Programm_Freies_Wissen/Einreichungen/Data_Version_Control:_Best_Practice_for_Reproducible,_Shareable_Science%3F/Book) and the raw data is available from [https://osf.io/vjxkq/](https://osf.io/vjxkq/)). In particular, I asked participants how they were versioning the different parts of their research projects, such as the manuscript, data analysis, but also the data itself. Interestingly, sophisticated data versioning practices (using GitHub or built-in functionalities of cloud storages) was prevalent for the manuscript itself (~50%) and the data analysis scripts (~66%). However, few researchers used Git for the data itself (n=3) and none of them were aware of specific tools for the versioning of larger data sets. Instead, several researchers voiced their interest in such tools and asked to be updated about this project in the future. In sum, the results indicate that version control of data is neither a prevalent practice nor do researchers know about the advantages and disadvantages of specific tools. This shows a gap in the way researchers version the different parts of their research projects.

## Which tools exist?

These findings presented a great motivation for my project because they indicated a clear need in the research community (of course, within the narrow sample of participants I asked). At the start of my research, I came across an old Stack Exchange post from 2013 with 16k views, which asked whether "Git for data" existed.

```{r git-for-data, echo=FALSE, fig.cap="'Is there a Git for data' on Stack Exchange (source: [https://opendata.stackexchange.com/questions/748/is-there-a-git-for-data](https://opendata.stackexchange.com/questions/748/is-there-a-git-for-data))",  fig.align="center", out.width="50%"}
knitr::include_graphics("figures/git_for_data.png")
```

Back then, Git for data wasn't something that really existed on the market. However, with the rise in data volume and, especially through the broad application of machine learning in many organizations, it became increasingly required to reproduce and share datasets. Thus, more recently, there have been a rise in open source tools that allow versions of datasets to be tracked over time. One comprehensive list of tools is available from: [https://docs.google.com/spreadsheets/d/1jGQY_wjj7dYVne6toyzmU7Ni0tfm-fUEmdh7Nw_ZH0k/edit?ts=5fc6a2a5#gid=0](https://docs.google.com/spreadsheets/d/1jGQY_wjj7dYVne6toyzmU7Ni0tfm-fUEmdh7Nw_ZH0k/edit?ts=5fc6a2a5#gid=0). Moreover, a discussion of these tools is available from: [https://www.youtube.com/watch?v=r5uxntl_hWg](https://www.youtube.com/watch?v=r5uxntl_hWg).

The available tools range from dedicated databases with version control capabilities (e.g., [DoltHub](https://github.com/dolthub/dolt), [TerminusDB](https://github.com/terminusdb/terminusdb)) to tools that are storage agnostic and allow for the versioning of the entire data pipeline (e.g., [DVC](https://github.com/iterative/dvc)).

## Which tools should researchers use?

It depends on the use case. As outlined above, there are different kinds of tools available. DoltHub, for example, is an open source SQL database with the ability to track differences in the data stored in the database. If multiple researchers are crawling data and each of them is submitting their piece to the database, such a solution might be ideal. However, in this case, researchers do not have the full flexibility as to where their data is stored. DVC, on the other hand, is a lightweight tool and storage agnostic, which only saves meta data about changes in the datasets. These files can then be tracked via Git. So, as you can see from these examples, researchers should first choose their area of application before choosing the solution.

Overall, as researchers have increasingly embraced Git to version control their research projects, 'Git for data'-tools are not on the radar of many researchers. In this first post, I gave a brief outline of the motivation and presented an overview of available products. Due to the heterogeneity in the tool landscape, I will be posting further in-depth discussions of different tools within this blog series. The first tool that I evaluate is DVC and the post is available [here](https://florianpethig.github.io/datavc/dvc).
